# Bernard Tool Call Streaming

> **Version**: 1.0.0  
> **Last Updated**: 2026-01-03  
> **Compatible With**: Bernard v2.0.0+

## Overview

Bernard now supports real-time streaming of tool calls. When streaming is enabled, clients receive:

1. **Tool call events** as they're generated by the LLM
2. **Tool progress events** from long-running operations
3. **Response tokens** as they're generated

This feature uses LangGraph's built-in streaming capabilities with the `messages` and `custom` stream modes, providing full visibility into the agent's decision-making process as it happens.

## Enabling Streaming

### Server-Side Configuration

Streaming is automatically enabled for requests to the `/v1/chat/completions` endpoint when `stream: true` is set in the request. The server configures the following stream modes:

```typescript
const streamResult = await graph.stream(
  { messages },
  {
    streamMode: ["messages", "updates", "custom"] as const
  }
);
```

### Client-Side Request

```typescript
const response = await fetch("/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    model: "bernard-v1",
    messages: [{ role: "user", content: "Search for weather in San Francisco" }],
    stream: true,
  }),
});

const reader = response.body?.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  console.log("Received:", chunk);
}
```

## Stream Events

The streaming response uses Server-Sent Events (SSE) format with `data:` prefixes. Each event contains JSON-formatted information about what's happening in the agent's execution.

### Event Types

| Event Type | Description | Fields |
|------------|-------------|--------|
| `tool_call` | Tool invocation with name and arguments | `id`, `type`, `function.name`, `function.arguments` |
| `content` | Response text token | `delta.content` |
| `progress` | Tool execution progress | `tool`, `phase`, `message`, `data` |
| `tool_result` | Result from tool execution | `role: "tool"`, `tool_call_id`, `content` |

### Tool Call Event

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "bernard-v1",
  "choices": [{
    "index": 0,
    "delta": {
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"San Francisco\"}"
        }
      }]
    },
    "finish_reason": null
  }]
}
```

### Content Token Event

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "bernard-v1",
  "choices": [{
    "index": 0,
    "delta": {
      "content": "The weather in San Francisco is currently"
    },
    "finish_reason": null
  }]
}
```

### Tool Progress Event

```json
{
  "type": "tool_progress",
  "request_id": "chatcmpl-abc123",
  "data": {
    "_type": "tool_progress",
    "tool": "web_search",
    "phase": "progress",
    "message": "2/3: Found 10 results",
    "timestamp": 1677858242500
  }
}
```

### Tool Result Event

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "bernard-v1",
  "choices": [{
    "index": 0,
    "delta": {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "content": "San Francisco weather: 65Â°F, partly cloudy"
    },
    "finish_reason": null
  }]
}
```

### Stream Completion

When the stream completes, a final chunk is sent with `finish_reason: "stop"`:

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "bernard-v1",
  "choices": [{
    "index": 0,
    "delta": {},
    "finish_reason": "stop"
  }]
}
data: [DONE]
```

## Stream Modes Reference

LangGraph supports multiple stream modes. Bernard uses the following:

| Mode | Output | Use For |
|------|--------|---------|
| `messages` | `[messageChunk, metadata]` | LLM tokens + tool calls |
| `updates` | `{ nodeName: { ...updates } }` | State deltas (debugging) |
| `custom` | Any user data | Tool progress events |
| `values` | Full state snapshot | Debugging |
| `debug` | Detailed trace | Debugging |

### Metadata Structure

When using `messages` mode, each chunk includes metadata:

```typescript
interface StreamMetadata {
  // Always present
  langgraph_node: string;      // e.g., "callModel"
  langgraph_path: string;      // e.g., "callModel -> tools"
  langgraph_step: number;      // e.g., 1
  
  // Present when LLM generates tool calls
  tool_calls?: Array<{
    id: string;                // Unique ID
    type: "function";
    function: {
      name: string;           // Tool name
      arguments: string;      // JSON args
    };
  }>;
  
  // Present when using tags
  tags?: string[];
  
  // Present for some models
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

## Tool Progress Reporting

Long-running tools can emit progress events to keep clients informed. This is useful for:

- Web searches with multiple result pages
- API calls with variable latency
- Multi-step operations

### Progress Event Types

| Phase | Description |
|-------|-------------|
| `step` | Starting a new step in the operation |
| `progress` | Reporting current progress (e.g., "5/10 items processed") |
| `complete` | Operation completed successfully |
| `error` | Operation failed with an error |

### Tools with Progress Reporting

The following tools support progress reporting:

- `web_search` - Reports search phases and result processing
- `wikipedia_search` - Reports search and content retrieval phases
- `wikipedia_entry` - Reports content extraction progress

Other tools execute quickly and don't require progress reporting.

## Migration Guide

### For OpenAI-Compatible Clients

**No changes required** - existing clients that work with OpenAI's streaming API will continue to work without modification. Bernard's streaming format is fully compatible with the OpenAI specification.

### For Internal Clients Using Old Format

If you were previously handling tool calls from the complete `AIMessage.tool_calls` field, update your stream handling to extract tool calls from metadata:

#### Before (Old Format)

```typescript
for await (const [mode, chunk] of stream) {
  if (mode === "messages") {
    const [message] = chunk;
    // Only message.content available
    // Tool calls only visible when message is complete
  }
}
```

#### After (New Format)

```typescript
for await (const [mode, chunk] of stream) {
  if (mode === "messages") {
    const [message, metadata] = chunk;
    
    // Message content for text
    if (typeof message.content === "string") {
      console.log("Text:", message.content);
    }
    
    // Tool calls from metadata (available as they're generated)
    if (metadata.tool_calls) {
      for (const toolCall of metadata.tool_calls) {
        console.log("Tool call:", toolCall.function.name);
      }
    }
  }
  
  if (mode === "custom") {
    // Tool progress events
    const event = chunk;
    if (event._type === "tool_progress") {
      console.log("Progress:", event.message);
    }
  }
}
```

### Handling Partial Arguments

Tool call arguments may arrive incrementally. Collect all chunks for a given tool call ID before parsing:

```typescript
const toolCallChunks: Record<string, string[]> = {};

for await (const [mode, chunk] of stream) {
  if (mode === "messages") {
    const [, metadata] = chunk;
    
    if (metadata.tool_calls) {
      for (const tc of metadata.tool_calls) {
        if (!toolCallChunks[tc.id]) {
          toolCallChunks[tc.id] = [];
        }
        toolCallChunks[tc.id].push(tc.function.arguments);
      }
    }
  }
}

// After stream completes, join arguments
const toolCalls = Object.entries(toolCallChunks).map(([id, chunks]) => ({
  id,
  name: /* extract from first chunk or store separately */,
  arguments: JSON.parse(chunks.join("")),
}));
```

## Example: Complete Streaming Client

```typescript
async function* streamChat(
  endpoint: string,
  messages: Array<{ role: string; content: string }>
): AsyncGenerator<StreamEvent> {
  const response = await fetch(`${endpoint}/v1/chat/completions`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "bernard-v1",
      messages,
      stream: true,
    }),
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();
  let buffer = "";

  while (reader) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split("\n");
    buffer = lines.pop() || "";

    for (const line of lines) {
      if (line.startsWith("data: ")) {
        const data = line.slice(6);
        if (data === "[DONE]") return;
        yield JSON.parse(data);
      }
    }
  }
}

// Usage
for await (const event of streamChat("http://localhost:8850", [
  { role: "user", content: "Search for Python tutorials" }
])) {
  if (event.choices?.[0]?.delta?.tool_calls) {
    console.log("Tool called:", event.choices[0].delta.tool_calls);
  }
  if (event.choices?.[0]?.delta?.content) {
    console.log("Content:", event.choices[0].delta.content);
  }
}
```

## Troubleshooting

### Tool Calls Not Appearing

If tool calls are not appearing in the stream:

1. Ensure you're using `streamMode: ["messages", "custom"]` (or similar)
2. Check that metadata is being extracted: `const [, metadata] = chunk`
3. Verify the graph was compiled with `ToolNode` for explicit tool execution

### Progress Events Not Received

Progress events require:

1. The tool to support progress reporting (web_search, wikipedia tools)
2. `custom` stream mode to be enabled
3. A `config.writer` to be available in the tool execution context

### Incomplete Arguments

If tool call arguments are truncated:

1. Arguments may arrive across multiple chunks
2. Collect all chunks for each tool call ID before parsing
3. Use `JSON.parse(chunks.join(""))` to reconstruct complete arguments

## Changelog

### v1.0.0 (2026-01-03)

- Initial release of tool call streaming
- Support for `messages`, `updates`, and `custom` stream modes
- Tool progress reporting for long-running operations
- Full OpenAI compatibility for streaming responses
