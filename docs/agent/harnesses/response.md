# Response Harness

The **Response Harness** is the final component in the conversation pipeline. Its job is to take the context, including any results from the router Harness, and generate a natural language response for the user.

## Purpose

While the router Harness focuses on *acting*, the Response Harness focuses on *communicating*. It ensures that the final output is helpful, polite, and incorporates all relevant information gathered during the turn.

## Features

- **Token Streaming**: Uses `streamText` to provide real-time updates to the user.
- **Fallback Logic**: If the LLM returns an empty response (often due to policy or technical errors), the harness can provide a helpful fallback based on the gathered context.
- **History Integration**: Uses the `Archivist` to ensure the response remains consistent with the conversation history.

## Standardized Events

The Response Harness emits the following `AgentOutputItem` events:

- `llm_call`: Emitted before starting the stream, containing the final context.
- `delta`: Emitted for every chunk of text generated by the LLM.
- `llm_call_complete`: Emitted once the stream is finished, containing the full aggregated response.
- `error`: Emitted if the streaming fails.

## FALLBACK BEHAVIOR

If the LLM fails to generate text or returns only whitespace, the Response Harness is designed to fail gracefully by examining the available tool results in the context. This prevents the user from receiving silent failures.

## Usage

```typescript
import { runResponseHarness } from "@/agent/harness/respond/responseHarness";

for await (const event of runResponseHarness(context)) {
    if (event.type === 'delta') {
        process.stdout.write(event.delta);
    }
}
```

## Testing

The harness is tested in `tests/responseHarness.test.ts`, verifying both successful streaming and error recovery.
